## 期望
**随机变量** 的 **期望**（*expectation*）是概率论中最重要的概念之一。如果 $X$ 是一个离散 **随机变量**，其可能的取值为 $x_1$，$x_2$，……那么 $X$ 的 **期望** 或 **期望值** $E[X]$ 为：

$$
E[X] = \sum_{i}{x_iP\{X = x_i\}}
$$ {#eq-4_4_001}

换句话说，$X$ 的 **期望** 是其可能取值的加权平均数，其中每个可能取值的权重为该值对应的概率。

例如，如果 $X$ 的概率质量函数为 $p(0)=p(1)=\frac{1}{2}$，则 $E[X]$ 只是 $X$ 的两个可能取值（0、1）的平均数：

$$
E[X] = 0 \cdot \frac{1}{2} + 1 \cdot \frac{1}{2} = \frac{1}{2}
$$

如果 $p(0)=\frac{1}{3}$，$p(1)=\frac{2}{3}$，则 $E[X]$ 是其两个可能取值（0、1）的加权平均值，其中 1 的权重是 0 的 2 倍（因为 $p(1)=2p(0)$）：

$$
E[X] = 0 \cdot \frac{1}{3} + 1 \cdot \frac{2}{3} = \frac{2}{3}
$$

我们也可以利用概率的频率解释（@sec-3_1）来定义随机变量的 **期望**。概率的评率解释认为，如果独立的重复执行一个实验无数次，那么对于任何事件 $E$，$E$ 发生的概率 $P(E)$ 就是其发生的次数的占比。现在，考虑一个随机变量 $X$，它的取值为 $x_1,x_2,...,x_n$，其概率分别为 $p(x_1),p(x_2),...,p(x_n)$。把 $X$ 想象成我们在一个 *机会游戏*（*game of chance*）中得到的奖金，也就是说，我们将赢得 $x_i$ 单位奖金的概率为 $p(x_i)$。根据概率的频率解释，如果我们继续玩这个游戏，那么我们赢得 $x_i$ 的次数占比将是 $p(x_i)$。因此，我们在 $n$ 次比赛中的平均奖金将是：

$$
\sum_{i=1}^{n}{x_ip(x_i)} = E[X]
$$ {#eq-4_4_002}

为了更清楚地看到 @eq-4_4_002 所示的结论，假设我们玩 $N$ 次游戏，其中 $N$ 是一个非常大的数。在这些游戏中，我们将赢得 $x_i$ 的次数大约为 $Np(x_i)$，因此我们在 $N$ 次比赛中的总奖金将是：

$$
\sum_{i=1}^{n}{x_i \cdot Np(x_i)}
$$

所以，我们可以获得的平均奖金为：

$$
\sum_{i=1}^{n}{\frac{x_i \cdot Np(x_i)}{N}} = \sum_{i=1}^{n}{x_ip(x_i)} = E[X]
$$

::: {#exr-4_4_a}
令 **随机变量** $X$ 为我们抛一个骰子所获得的点数，计算 $E[X]$。
:::

::: {#sol-4_4_a}
$$
\begin{align}
\because\ & p(1)=p(2)=p(3)=p(4)=p(5)=p(6)=\frac{1}{6} \\
\therefore\ & E[X] = 1 \cdot \frac{1}{6} + 2 \cdot \frac{1}{6} + 3 \cdot \frac{1}{6} + 4 \cdot \frac{1}{6} + 5 \cdot \frac{1}{6} + 6 \cdot \frac{1}{6} = \frac{7}{2}
\end{align}
$$

读者应该注意，在 @exr-4_4_a 中，$X$ 的期望值并不是 $X$ 的可能取值（也就是说，抛一个骰子，我们不可能得到 \frac{7}{2} 的点数）。因此，即使我们称 $E[X]$ 为 $X$ 的期望，我们也不能应该把 $E[X]$ 看作我们期望 $X$ 具有的值，而应该看作在大量重复实验中 $X$ 的平均值。也就是说，如果我们连续抛一个骰子，那么在抛了很多次之后，所得结果的平均点数将大约是 $\frac{7}{2}$。（感兴趣的读者可以尝试一下这个实验。）$\blacksquare$
:::

::: {#exm-4_4_b}
如果一个事件 $A$ 的 **指示随机变量** 为 $I$，如果

$$
I = \begin{cases}
1, \quad & 如果 A 发生\\
0, \quad & 如果 A 不发生
\end{cases}
$$

则 $E[I] = 1 \cdot P(A) + 0 \cdot P(A^c) = P(A)$。因此，**指示随机变量** 的期望就是对应的事件发生的概率。$\blacksquare$
:::

::: {#exm-4_4-c}
*熵（Entropy）*。给定一个 **随机变量** $X$，$X=x$ 传递了多少信息（*information*）？让我们通过如下的方式来开始尝试量化 $X=x$ 中所传递的信息：$X=x$ 的信息量应该取决于 $X$ 等于 $x$ 的可能性。$X$ 等于 $x$ 的可能性越小，他所包含的信息就越多，这看起来是合理的。例如：如果 $X$ 表示抛两个骰子的点数之和，则 $X = 12$ 包含的信息量比 $X = 7$ 包含的信息量要大（因为 $P\{X=12\}=\frac{1}{36}$，而 $P\{X=7\}=\frac{1}{6}$）。

让我们用 $I(p)$ 来表示一个概率为 $p$ 的事件发生时的信息量。显然，$I(p)$ 应该是 $p$ 的非负递减函数。为了确定 $I(p)$，令 $X$ 和 $Y$ 是独立 **随机变量**，假设 $P\{X=x\}=p$、$P\{Y=y\}=q$。那么 $X=x$、$Y=y$ 包含多少信息量？

* 首先，$X=x$ 中包含的信息量是 $I(p)$。
* 此外，由于我们已经知道 $Y=y$ 的概率并不受 $X=x$ 的影响（因为 $X$ 和 $Y$ 是独立的），所以 $Y=y$ 中包含的额外信息量应该等于 $I(q)。
* 因此，$X=x$、$Y=y$ 中的信息量为 $I(p)+I(q)$。
* 另一方面，我们有 $P\{X=x, Y=y\}=P\{X=x\}P\{Y=y\}=pq$，这意味着 $X=x$、$Y=y$ 中的信息量为 $I(pq)$。
* 因此，看起来，$I$ 应该满足：$I(pq) = I(p) + I(q)$。
* 如果我们定义函数 $G$ 为：$G(p)=I(2^{-p})$，于是有：
  
  $$
  \begin{align}
  G(p+q) &= I(2^{-(p+q)}) \\
  &= I(2^{-p} \cdot 2^{-q}) \\
  &= I(2^{-p}) + I(2^{-q}) \\
  &= G(p) + G(q)
  \end{align}
  $$

  可以证明，满足上述关系的唯一的单调函数 $G$ 是 $G(p) = cp$，其中 $c$ 为常数。
* 于是，我们有 $I(2^{-p}) = cp$，令 $q=2^{-p}$，则 $I(q) = -c \log_2{q}$（其中，$c$ 是一个大于 0 的常数）。我们经常令 $c=1$，并且说：用 **位（*bits*）** 来测量信息量。

现在，考虑一个随机变量 $X$，它的取值为 $x_1,x_2,...,x_n$，其概率分别为 $p_1,p_2,...,p_n$。因为 $X=x$ 的信息量为 $-\log_2(p_i)$，因此，$X=x_i$ 所包含的信息量的 **期望** 为：

$$
H(X) = - \sum_{i=1}^{n}{p_i \log_2{p_i}}
$$

在信息论中，$H(X)$ 就是众所周知的 **随机变量** $X$ 的 **熵（*entropy*）**。 $\blacksquare$
:::

我们还可以定义连续 **随机变量** 的 **期望**。假设 $X$ 是一个概率密度函数为 $f$ 的连续 **随机变量**。对于一个较小的数 $\Delta x$，有：

$$
f(x) \Delta x \approx P\{x \lt X \lt x + \Delta x\}
$$

如果 $x$ 的权重等于 $X$ 在 $x$ 附近的概率，则 $X$ 的所有可能值的加权平均数就是 $xf(x) \Delta x$ 在所有 $x$ 上的积分。通常，定义 $X$ 的期望为：

$$
E[X] = \int_{-\infty}^{\infty}{xf(x) \mathrm{d} x}
$$

::: {#exm-4_4_d}
假设您在下午 5 点后的某个时间期待一条消息。根据经验，您知道在下午 5 点后的 $X$ 个小时内会收到消息，并且 $X$ 是一个 **随机变量**，其概率密度函数为：

$$
f(x) = \begin{cases}
\frac{1}{1.5}, \quad & 0 \lt x \lt 1.5 \\
0, \quad & otherwise
\end{cases}
$$

则下午 5 点之后可以收到消息的期望时间为：$E[X] = \int_{0}^{1.5}{\frac{1}{1.5} \mathrm{d} x} = 0.75$。因此，您等待消息的时间平均为 45 分钟。  $\blacksquare$
:::

::: {.callout-tip title="备注"}
* **期望** 的概念类似于物理中的 **重心** 的概念。考虑一个离散 **随机变量** $X$，其概率质量函数为 $p(x_i), i \ge 1$。如果我们现在想象一个质量可以忽略不计的杆（*weightless rod*），在这根杆上的 $x_i$ 处的质量为 $p(x_i)$（$i \ge 1$)（见 @fig-4_4）。那么这根杆处于平衡状态的点称为重心（*the center fo gravity*）。对于那些熟悉统计学基础的读者来说，很容易证明这根杆的重心位于 $E[X]$ [^2]。
  
  ![重心位于 0.9 处](../images/chapter_4/f_4_4.png){#fig-4_4}
* $E[X]$ 的计量单位与 $X$ 相同。
:::

[^2]: 为了证明这一点，我们必须证明这根杆绕点 $E[X]$ 转动的力矩之和等于 0。也就是说，我们必须证明 $0=\sum_{i}{(x_i−E[X])p(x_i)}$。