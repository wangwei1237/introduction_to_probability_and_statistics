## 关于回归参数的统计推断 {#sec-9_4}
使用 @prp-9_3_1，设计回归参数的假设检验和置信区间是一件非常简单的事情。

### 关于 $\beta$ 的统计推断 {#sec-9_4_1}
$\beta = 0$ 是简单线性回归模型 $Y = \alpha + \beta x + e$ 的一个重要的假设。该假设的重要性源自这样的事实，即 $\beta = 0$ 相当于声明模型的输出均值不依赖于模型的输入，或者说没有对输入变量进行任何的回归。

为了检验：

$H_0: \beta = 0 \quad vs. \quad H_1: \beta \ne 0$

我们注意到，根据 @prp-9_3_1 有：

$$
\frac{B - \beta}{\sqrt{\sigma^2 / S_{xx}}} = \sqrt{S_{xx}} \frac{B - \beta}{\sigma} \sim \mathcal{N}(0, 1)
$$ {#eq-9_4_1}

同时，残差平方和 $SS_R$ 于 $B$ 之间相互独立，且：

$$
\frac{SS_R}{\sigma^2} \sim \chi_{n-2}^{2}
$$

因此，根据 $t$ 分布随机变量的定义，

$$
\frac{\sqrt{S_{xx}}(B - \beta) / \sigma}{\sqrt{\frac{SS_R}{\sigma^2(n-2)}}} = \sqrt{\frac{(n-2)S_{xx}}{SS_R}}(B - \beta) \sim t_{n-2}
$$ {#eq-9_4_2}

也就是说，$\sqrt{\frac{(n-2)S_{xx}}{SS_R}}(B - \beta)$ 服从自由度为 $n-2$ 的 $t$ 分布。因此，如果 $H_0$ 为真（即 $\beta = 0$），则：

$$
\sqrt{\frac{(n-2)S_{xx}}{SS_R}}B \sim t_{n-2}
$$

**$H_0: \beta = 0$ 的假设检验**
$H_0$ 在显著性水平 $\gamma$ 下的检验为：

**拒绝：** $H_0 \quad 如果 \quad \sqrt{\frac{(n-2)S_{xx}}{SS_R}} |B| \gt t_{\gamma/2, n-2}$

**接受：** $H_0 \quad 其他$

为了计算该检验，首先计算检验统计量 $\sqrt{(n-2)S_{XX}/SS_R}|B|$ 的值（$v$），然后如果如下所示的 $p \text{-value}$ 小于给定的显著性水平则拒绝 $H_0$：

$p \text{-value} = P\{|T_{n-2}| > v\} = 2P\{T_{n-2} > v\}$

其中 $T_{n-2}$ 服从自由度为 $n-2$ 的 t 分布随机变量。

可以使用 R 来计算如上的假设检验。例如，如果模型的名称为 `fit`，那么 `summary(fit)` 将提供所需的 $p \text{-value}$。

::: {#exr-9_4_a}
一个人声称他的汽车的燃油消耗量不依赖于汽车的行驶速度。为了检验这个假设的合理性，在不同的速度（45~70 英里/小时）测试了汽车。不同速度下的每加仑英里的测试数据如下：

| 速度 | 每加仑英里数 |
|-------------|------------------|
| 45          | 24.2             |
| 50          | 25.0             |
| 55          | 23.3             |
| 60          | 22.0             |
| 65          | 21.5             |
| 70          | 20.6             |
| 75          | 19.8             |

这些数据是否拒绝了每加仑汽油的英里数不受汽车行驶速度影响的说法？
:::

::: {#sol-9_4_a}
假设 $Y$ 为汽车每加仑汽油的英里数，$x$ 为汽车的行驶速度，并且 $Y$ 和 $x$ 符合简单线性回归模型：

$Y = \alpha + \beta x + e$

因此，题目中的假设意味着回归系数 $\beta = 0$。

使用 R，我们有：

```{r}
#| code-fold: false

x <- c(45, 50, 55, 60, 65, 70, 75)
y <- c(24.2, 25, 23.3, 22, 21.5, 20.6, 19.8)
miles <- lm(y ~ x)
miles

summary(miles)
```

`summary(miles)` 首先给出七个数据的残差 $y_i - (A + Bx_i)$，其中 $i = 1, \ldots, 7$。接下来的几行给出了截距 $\alpha$ 和斜率 $\beta$ 的估计值分别为 32.54286 和 -0.17。然后，给出了这些估计值的标准误差。接着给出了用于检验回归参数为零的 $t$-统计量的值。例如，检验 $\beta = 0$ 的 $t$-值为 -8.138。随后给出了回归参数为零的检验的 $p \text{-value}$，例如：$\alpha = 0$ 的 $p \text{-value}$ 为 $1.69 \times 10^{-6}$，$\beta = 0$ 的 $p \text{-value}$ 为 0.000455，这表明在实践中，无论显著性水平为何值，两个假设都将被拒绝。残差标准误差的值为 $\sqrt{\frac{SS_R}{n-2}}$。由于 $SS_R/(n-2)$ 是 $\sigma^2$ 的估计量，因此残差标准误差是 $\sigma$ 的估计量。 $\blacksquare$
:::

根据 @eq-9_4_2 可以很容易地得到 $\beta$ 的置信区间估计。实际上，从 @eq-9_4_2 可以推导出，对于任意 $a$，其中 $0 < a < 1$，有

$P \left\{ -t_{a/2, n-2} < \sqrt{\frac{(n-2)S_{xx}}{SS_R}} (B - \beta) < t_{a/2, n-2} \right\} = 1 - a$

即：

$P \left\{ B - \sqrt{\frac{SS_R}{(n-2)S_{xx}}} t_{a/2, n-2} < \beta < B + \sqrt{\frac{SS_R}{(n-2)S_{xx}}} t_{a/2, n-2} \right\} = 1 - a$

于是，我们得到了以下的结果。

**$\beta$ 的置信区间**

$\beta$ 的 $100(1 - a) \%$ 的置信区间估计是：

$\left( B - \sqrt{\frac{SS_R}{(n-2)S_{xx}}} t_{a/2, n-2}, B + \sqrt{\frac{SS_R}{(n-2)S_{xx}}} t_{a/2, n-2} \right)$

::: {.callout-tip title="备注"}
因为 $\sigma^2$ 未知，所以不能直接使用 $\frac{B - \beta}{\sqrt{\sigma^2/S_{xx}}} \sim \mathcal{N}(0,1)$ 来对 $\beta$ 进行统计推断。相反，我们采用了 @sec-9_3 中提到的 $\sigma^2$ 的估计量 $SS_R/(n−2)$，这导致统计量的分布从标准正态分布变为自由度为 $n−2$ 的 $t$ 分布。
:::

::: {#exm-9_4_b}
我们还可以使用 R 来获得 $\alpha$ 和 $\beta$ 的置信区间。如果模型的名称是 `name`，那么 R 命令 `confint(name, level = m)` 会返回模型 `name` 中的 $\alpha$ 和 $\beta$ 的 $100m \%$ 置信区间。例如，假设我们想要计算模型 `miles` 的参数的 95% 置信区间。如果我们输入 `confint(miles, level = 0.95)`，那么 R 会返回 $\alpha$ 和 $\beta$ 的 95% 置信区间。

```{r}
#| code-fold: false

x <- c(45, 50, 55, 60, 65, 70, 75)
y <- c(24.2, 25, 23.3, 22, 21.5, 20.6, 19.8)
miles <- lm(y ~ x)
confint(miles)
```

所以 $\alpha$ 和 $\beta$ 的 95% 置信区间分别为：$(29.2767, 35.8090)$ 和 $(−0.2237, −0.1163)$。
:::

### 均值回归 {#sec-9_4_2}
术语 **回归**（*regression*）最初由 Francis Galton 在描述遗传定律时使用。Galton 认为这些定律导致了种群中的极端值“向均值回归”（*regress toward the mean*）。在“向均值回归”中，Galton 的意思是：在某个特征上具有极端值的人的孩子会倾向于比他们的父母具有更不极端的值。

如果我们假设在某特征上，后代 ($Y$) 和父代 ($x$) 之间存在线性回归关系，那么当回归参数 $\beta$ 介于 0 和 1 之间时，将发生均值回归。也就是说，如果

$E[Y] = \alpha + \beta x$

并且 $0 < \beta < 1$，那么当 $x$ 很大时，$E[Y]$ 将小于 $x$；当 $x$ 很小时，$E[Y]$ 将大于 $x$。可以通过代数方法或者绘制两条直线 $y = \alpha + \beta x$ 和 $y = x$ 来轻松验证如上的陈述。

在直线图中我们可以发现，当 $0 < \beta < 1$ 时，对于较小的 $x$ 值，$y = \alpha + \beta x$ 在 $y = x$ 的上方，而对于较大的 $x$ 值，则位于其下方。

::: {#exm-9_4_c}
为了演示 Galton 的均值回归理论，英国统计学家 Karl Pearson 绘制了 10 名随机选择的男孩与其父亲的身高的对比图。具体数据（以英寸为单位）如下：

父亲的身高：60,   62,   64, 65,   66,   67,   68,   70,   72,   74

儿子的身高：63.6, 65.2, 66, 65.5, 66.9, 67.1, 67.4, 68.3, 70.1, 70

如上数据的散点图如 @fig-9_3 所示。

```{r}
#| label: fig-9_3
#| fig-cap: "父子之间的身高对比图"
#| warning: false
#| 

father_height <- c(60, 62, 64, 65, 66, 67, 68, 70, 72, 74)
son_height <- c(63.6, 65.2, 66, 65.5, 66.9, 67.1, 67.4, 68.3, 70.1, 70)
plot(father_height, son_height)
```

注意，虽然数据表明身高较高的父亲其儿子的身高也往往较高，但也表明较高或者较低身高的父亲的儿子的身高往往比他们更“平均”——也就是说，孩子的身高有“向均值回归”的现象。

我们将通过将 **均值回归** 作为 **备则假设** 来确定上述数据是否足以证明 **均值回归** 的存在。也就是说，我们将使用上述数据来检验：

$H_0: \beta \geq 1 \quad \text{vs} \quad H_1: \beta < 1$

即：

$H_0: \beta = 1 \quad \text{vs} \quad H_1: \beta < 1$

根据 @eq-9_4_2，当 $\beta = 1$ 时，检验统计量：

$TS = \sqrt{\frac{8S_{xx}}{SS_R}}(B-1) \sim t_{8}$ 

如果 $TS = v$，则原假设 $H_0: \beta \ge 1$ 的 $p \text{-value}$ 为：

$p \text{-value} = P\{T_8 \le v\}$

其中，$T_8$ 为自由度为 8 的 $t$-分布。使用 R 来计算如上的值：

```{r}
#| code-fold: false

x <- c(60, 62, 64, 65, 66, 67, 68, 70, 72, 74)
y <- c(63.6, 65.2, 66, 65.5, 66.9, 67.1, 67.4, 68.3, 70.1, 70)
height = lm(y ~ x)

summary(height)
```

由此可知，$\beta$ 的估计值 $B = 0.46457$，并且 $\beta$ 的估计的标准误差为 0.03298，$\beta = 0$ 时的统计检验 $\sqrt{\frac{8S_{xx}}{SS_R}}B$ 为 14.08。因此，对于 $H_0: \beta = 1$ 时的统计检验值 $TS=\sqrt{\frac{8S_{xx}}{SS_R}}(B-1) = 14.08 - \frac{14.08}{0.46457} = -16.2276$。

```{r}
#| code-fold: false

pt(-16.2276, 8)
```

因此，统计检验量的值是 -16.2276，相应的 $p \text{-value}$ 是 $1.045569 \times 10^{-7}$。因此，在任何显著性水平上，原假设 $\beta \geq 1\$ 将被拒绝，从而建立了均值回归。（参见 @fig-9_4，在 @fig-9_3 的散点图上增加了直线 $y = x$）。

```{r}
#| label: fig-9_4
#| fig-cap: "对于比较小的 x，y > x；对于比较大的 x，y < x"
#| warning: false
#| 

father_height <- c(60, 62, 64, 65, 66, 67, 68, 70, 72, 74)
son_height <- c(63.6, 65.2, 66, 65.5, 66.9, 67.1, 67.4, 68.3, 70.1, 70)
plot(father_height, son_height, xlim=c(60, 74), ylim=c(60, 74))
abline(a=0, b=1, col="red")
```

一种现代生物学解释认为，均值回归现象大致可以解释为：后代会随机获取父母的一半的基因，因此，假设某人的身高非常高，则其后代很可能由于偶然原因拥有了更少的“高个子”基因。

尽管均值回归现象最重要的应用是关于后代与其父母的生物学特征之间的关系，但这一现象在我们拥有指向相同变量的两组数据场景时也会出现。$\blacksquare$
:::

::: {#exm-9_4_d}
| 县市 | 1988 年死亡人数 | 1989 死亡人数 |
|--------|----------------|----------------|
| 1      | 121            | 104            |
| 2      | 96             | 91             |
| 3      | 85             | 101            |
| 4      | 113            | 110            |
| 5      | 102            | 117            |
| 6      | 118            | 108            |
| 7      | 90             | 96             |
| 8      | 84             | 102            |
| 9      | 107            | 114            |
| 10     | 112            | 96             |
| 11     | 95             | 488            |
| 12     | 101            | 106            |

: 美国西北部 12 个县在 1988 年和 1989 年的机动车死亡人数 {#tbl-9_1}

@tbl-9_1 的数据涉及美国西北部 12 个县在 1988 年和 1989 年发生的机动车死亡人数。

```{r}
#| label: fig-9_5
#| fig-cap: "1988 年和 1989 年的死亡人数对比图"
#| warning: false

death_1988 <- c(121, 96, 85, 113, 102, 118, 90, 84, 107, 112, 95, 101)
death_1989 <- c(104, 91, 101, 110, 117, 108, 96, 102, 114, 96, 488, 106)
plot(death_1988, death_1989, xlim = c(80, 130), ylim = c(80, 130))
```

从 @fig-9_5 可以看出，1989 年大多数县的死亡人数减少了，这些县在 1988 年有大量的机动车死亡人数。同样，那些在 1988 年死亡人数较少的县，1989 年的死亡人数似乎有所增加。因此，我们可以认为均值回归现象确实存在。实际上，可以用 R 计算 @tbl-9_1 的估计回归方程：

$y = 74.589 + 0.276x$

这也表明，$\beta$ 的估计值确实小于 1。

在考虑上述数据中的均值回归现象的原因时，我们必须要小心。例如，我们会很自然的假设，在 1988 年机动车死亡人数较多的县可能通过改善道路安全或让人们更多地意识到不安全驾驶的潜在危险来做出大力改进，以减少机动车死亡人数。同时，我们可能假设在 1988 年死亡人数最少的那些县可能“沾沾自喜”，并且没有做出更多的努力来进一步改善机动车的死亡人数，因此导致第二年死亡人数的增加。

尽管这种假设可能是正确的，但重要的是要认识到，即使没有任何一个县做出了任何非同寻常的举措，也可能会发生均值回归现象。事实上，很有可能在 1988 年那些死亡人数较多的县在那一年非常不幸，因此下一年的减少只是其更正常结果的回归。例如，如果抛 10 次硬币，如果出现了 9 次正面朝上，那么再抛这枚硬币 10 次，则正面向上的次数很可能会少于 9 次。同样，那些在 1988 年死亡人数较少的县在那一年可能非常“幸运”，而 1989 年的更正常结果将导致死亡人数的增加。

错误地认为均值回归是由于某种外部影响导致，但实际上均值回归只是由于统计中的随机性（*chance occurs frequently*）而导致的，这通常被称为 **回归谬误**（*the regression fallacy*）。$\blacksquare$
:::

### 关于 $\alpha$ 的统计推断 {#sec-9_4_3}
确定 $\alpha$ 的置信区间和假设检验的方法与 $\beta$ 的方法完全相同。具体而言，可利用 @prp-9_3_1 证明：

$$
\sqrt{\frac{n(n-2)S_{xx}}{SS_R \sum_i x_i^2}} (A - \alpha) \sim t_{n-2}
$$ {#eq-9_4_3}

**$\alpha$ 的置信区间估计**

$\alpha$ 的 $100(1 - \alpha) \%$ 的置信区间为：

$$
A \pm t_{\alpha/2, n-2} \sqrt{\frac{SS_R \sum_i x_i^2}{n(n-2)S_{xx}}}
$$

关于 $\alpha$ 的假设检验可以很容易地由 @eq-9_4_3 得到，其推导过程将留作习题。

### 关于 $\alpha + \beta x_0$ 的统计推断 {#sec-9_4_4}

