## 估计量的分布 {#sec-9_3}
为了确定估计量 $A$ 和 $B$ 的分布，不能仅仅假设随机误差的均值为 0，我们有必要对随机误差做出额外的假设。通常，我们假设随机误差是均值为 0，方差为 $\sigma^2$ 的独立的、正态分布随机变量。也就是说，如果输入值 $x_i$ 对应的输出为 $Y_i$，那么 $Y_1, \ldots, Y_n$ 相互独立，并且：

$Y_i \sim \mathcal{N}(\alpha + \beta x_i, \sigma^2)$

注意，上述假设认为随机误差的方差不依赖于输入值，而是一个常数。并且，我们假设 $\sigma^2$ 的值是未知的，我们必须通过数据对 $\sigma^2$ 进行估计。

根据 @prp-9_2_1，$\beta$ 的最小二乘估计量 $B$ 可以表示为：

$$
B = \frac{\sum_{i}(x_i - \overline{x}) Y_i}{\sum_{i} x_i^2 - n\overline{x}^2}
$$ {#eq-9_3_1}

因此，$B$ 是独立、正态分布随机变量 $Y_i, i = 1, \ldots, n$ 的线性组合，所以 $B$ 本身也服从正态分布。根据 @eq-9_3_1，$B$ 的均值和方差为：

$$
\begin{align}
E[B] &= \frac{\sum_{i}(x_i - \overline{x}) E[Y_i]}{\sum_{i} x_i^2 - n\overline{x}^2} \\
&= \frac{\sum_{i}(x_i - \overline{x})(\alpha + \beta x_i)}{\sum_{i} x_i^2 - n\overline{x}^2} \\
&= \frac{\alpha \sum_{i}(x_i - \overline{x}) + \beta \sum_{i}x_i(x_i - \overline{x})}{\sum_{i} x_i^2 - n\overline{x}^2} \\
&= \beta \frac{\sum_i x_i^2 - \overline{x}\sum_i x_i}{\sum_i x_i^2 - n\overline{x}^2} \quad \because \sum_i(x_i - \overline{x}) = 0 \\
&= \beta
\end{align}
$$

因此，$E[B] = \beta$，所以 $B$ 是 $\beta$ 的无偏估计量。现在，我们计算 $B$ 的方差。

$$
\begin{align}
\textup{Var}(B) &= \frac{\textup{Var}\bigg( \sum_{i=1}^n (x_i - \overline{x}) Y_i \bigg)}{\bigg( \sum_{i=1}^n x_i^2 - n\overline{x}^2 \bigg)^2} \\
&= \frac{\sum_{i=1}^n (x_i - \overline{x})^2 \textup{Var}(Y_i)}{\bigg( \sum_{i=1}^n x_i^2 - n \overline{x}^2 \bigg)^2} \text{ by independence} \\
&= \frac{\sigma^2 \sum_{i=1}^n (x_i - \overline{x})^2}{\bigg( \sum_{i=1}^n x_i^2 - n \overline{x}^2 \bigg)^2} \\
&= \frac{\sigma^2}{\sum_{i=1}^n x_i^2 - n \overline{x}^2} \quad \because \sum_{i=1}^n (x_i - \overline{x})^2 = \sum_{i=1}^n x_i^2 - n \overline{x}^2
\end{align}
$$ {#eq-9_3_2}

根据 @eq-9_3_1 以及 $A = \frac{\sum_{i=1}^n Y_i}{n} - B \overline{x}$ 表明 $A$ 也可以表示为独立正态分布随机变量 $Y_i, i = 1, \ldots, n$ 的线性组合，因此 $A$ 也服从正态分布。$A$ 的均值为：

$$
\begin{align}
E[A] &= \sum_{i=1}^n \frac{E[Y_i]}{n} - \overline{x} E[B] \\
&= \sum_{i=1}^n \frac{\alpha + \beta x_i}{n} - \overline{x} \beta \\
&= \alpha + \beta \overline{x} - \overline{x} \beta \\
&= \alpha
\end{align}
$$ 

因此，$A$ 也是 $\alpha$ 的无偏估计量。首先将 $A$ 表示为 $Y_i$ 的线性组合来计算 $A$ 的方差，其结果为（详细的推导步骤作为练习留给读者）：

$$
\text{Var}(A) = \frac{\sigma^2 \sum_{i=1}^n x_i^2}{n \bigg( \sum_{i=1}^n x_i^2 - n \overline{x}^2 \bigg)}
$$ {#eq-9_3_3}

$Y_i - A - Bx_i, i = 1, \ldots, n$，表示实际值（即 $Y_i$）与其最小二乘估计量（即 $A + Bx_i$）之间的差异，我们称之为 **残差**（*residuals*）。可以用残差的平方和：

$SS_R = \sum_{i=1}^n (Y_i - A - Bx_i)^2$

来估计随机误差的方差 $\sigma^2$。实际上，可以证明

$\frac{SS_R}{\sigma^2} \sim \chi^2_{n-2}$

也就是说，$\frac{SS_R}{\sigma^2}$ 服从自由度为 $n-2$ 的卡方分布，这意味着

$E \left[ \frac{SS_R}{\sigma^2} \right] = n-2$

或者

$E \left[ \frac{SS_R}{n-2} \right] = \sigma^2$

因此，$\frac{SS_R}{n-2}$ 是 $\sigma^2$ 的无偏估计量。此外，还可以证明 $SS_R$ 与 $A$ 和 $B$ 是独立的。

::: {.callout-tip title="备注"}
关于为什么 $\frac{SS_R}{\sigma^2}$ 服从自由度为 $n-2$ 的卡方分布并且与 $A$ 和 $B$ 独立的合理性论证如下。

因为 $Y_i$ 是独立正态随机变量，因此 $\frac{(Y_i - E[Y_i])}{\sqrt{\text{Var}(Y_i)}}$（$i = 1, \ldots, n$）服从标准正态分布，所以：

$\sum_{i=1}^n \frac{(Y_i - E[Y_i])^2}{\text{Var}(Y_i)} = \sum_{i=1}^n \frac{(Y_i - \alpha - \beta x_i)^2}{\sigma^2} \sim \chi^2_n$

现在，如果我们用估计量 $A$ 和 $B$ 替代 $\alpha$ 和 $\beta$，那么将失去 2 个自由度，因此 $\frac{SS_R}{\sigma^2}$ 服从自由度为 $n-2$ 的卡方分布并不令人惊讶。

事实上，$SS_R$ 与 $A$ 和 $B$ 的独立性与一个基本结果非常相似，即在正态分布抽样中，$\overline{X}$ 和 $S^2$ 是独立的（@thm-6_5_1）。实际上，$\overline{X}$ 和 $S^2$ 的独立性表明，如果 $Y_1, \ldots, Y_n$ 是均值为 $\mu$、方差为 $\sigma^2$ 的正态分布样本，那么平方和 $\sum_{i=1}^n (Y_i - \mu)^2 / \sigma^2$ 服从自由度为 $n$ 的卡方分布。如果用均值的估计量 $\overline{Y}$ 替代 $\mu$ 得到新的平方和 $\sum_{i} (Y_i - \overline{Y})^2 / \sigma^2$，那么这个量（等于 $(n-1)S^2 / \sigma^2$）将与 $\overline{Y}$ 独立，并且将服从自由度为 $n-1$ 的卡方分布。由于 $SS_R / \sigma^2$ 是通过在平方和 $\sum_{i=1}^n (Y_i - \alpha - \beta x_i)^2 / \sigma^2$ 中用估计量 $A$ 和 $B$ 来分别替代 $\alpha$ 和 $\beta$，因此这个量可能与 $A$ 和 $B$ 是独立的。

当 $Y_i$ 是正态分布随机变量时，最小二乘估计量也是最大似然估计量。为了验证这一点，注意到 $Y_1, \ldots, Y_n$ 的联合密度由下式给出：

$$
\begin{align}
f_{Y_1, \ldots, Y_n}(y_1, \ldots, y_n) &= \prod_{i=1}^n f_{Y_i}(y_i) \\
&= \prod_{i=1}^n \frac{1}{\sqrt{2\pi} \sigma} e^{-(y_i - \alpha - \beta x_i)^2 / 2\sigma^2} \\
&= \frac{1}{(2\pi)^{n/2} \sigma^n} e^{-\sum_{i=1}^n (y_i - \alpha - \beta x_i)^2 / 2\sigma^2} \\
\end{align}
$$

因此，$\alpha$ 和 $\beta$ 的最大似然估计量恰好是使 $\sum_{i=1}^n (y_i - \alpha - \beta x_i)^2$ 最小的值，即最小二乘估计量。
:::

::: {.callout-tip title="符号说明"}
如果我们令：

$S_{xY} = \sum_{i=1}^n (x_i - \overline{x})(Y_i - \overline{Y}) = \sum_{i=1}^n x_i Y_i - n \overline{x} \overline{Y}$

$S_{xx} = \sum_{i=1}^n (x_i - \overline{x})^2 = \sum_{i=1}^n x_i^2 - n \overline{x}^2$

$S_{YY} = \sum_{i=1}^n (Y_i - \overline{Y})^2 = \sum_{i=1}^n Y_i^2 - n \overline{Y}^2$

则最小二乘估计量可以表示为：

$B = \frac{S_{xY}}{S_{xx}}$

$A = \overline{Y} - B\overline{x}$

因此，我们得到如下的残差的平方和的计算等式：

$$
SS_R = \frac{S_{xx}S_{YY} - S_{xY}^2}{S_{xx}}
$$ {#eq-9_3_4}
:::

@prp-9_3_1 对本节的内容进行了总结。

::: {#prp-9_3_1}
假设因变量 $Y_i, i = 1, \ldots, n$ 是均值为 $\alpha + \beta x_i$、方差为 $\sigma^2$ 的独立的、正态分布随机变量。$\beta$ 和 $\alpha$ 的最小二乘估计量为：

$B = \frac{S_{xY}}{S_{xx}}, \quad A = \overline{Y} - B\overline{x}$

其分布为：

$A \sim \mathcal{N} \left( \alpha, \frac{\sigma^2 \sum_{i} x_i^2}{n S_{xx}} \right)$

$B \sim \mathcal{N} \left( \beta, \frac{\sigma^2}{S_{xx}} \right)$

此外，如果我们令：

$SS_R = \sum_{i} (Y_i - A - Bx_i)^2$

表示残差平方和，那么：

$\frac{SS_R}{\sigma^2} \sim \chi^2_{n-2}$

并且 $SS_R$ 与最小二乘估计量 $A$ 和 $B$ 是独立的。同时，$SS_R$ 可以通过如下的方式来计算：

$SS_R = \frac{S_{xx}S_{YY} - (S_{xY})^2}{S_{xx}}$
:::

可以使用 R 计算 $S_{xY}$，$S_{xx}$，$S_{YY}$，$SS_R$：

```{.r}
x <- c(x_1, ..., x_n)
y <- c(y_1, ..., y_n)
Sxy <- sum(x * y) - n * mean(x) * mean(y)
Sxx <- sum(x * x) - n * mean(x)^2
Syy <- sum(y * y) - n * mean(y)^2
SSR <- (Sxx * Syy - Sxy^2) / Sxx
```

::: {#exr-9_3_a}
下面的数据中，$x$ 为某种产品的湿混合物的水分，$Y$ 为最终产品的密度。

| $x_i$ | $y_i$ |
| --- | --- |
| 5 | 7.4 |
| 6 | 9.3 |
| 7 | 10.6 |
| 10 | 15.4 |
| 12 | 18.1 |
| 15 | 22.2 |
| 18 | 24.1 |
| 20 | 24.8 |

为这些数据拟合一条线性曲线，并确定回归方程的残差的平方和 $SS_R$。
:::

::: {#sol-q_3_a}
使用 R 计算 $SS_R$：

```{r}
#| code-fold: false

x <- c(5,6,7,10,12,15,18,20)
y <- c(7.4, 9.3, 10.6, 15.4, 18.1, 22.2, 24.1, 24.8)
Sxy = sum(x * y) - 8 * mean(x) * mean(y)
Sxx = sum(x * x) - 8 * mean(x)^2
Syy = sum(y * y) - 8 * mean(y)^2
SSR = (Sxx * Syy - Sxy^2) / Sxx
SSR
```

使用 R 求线性回归方程：

```{r}
#| label: fig-9_2
#| fig-cap: "@exr-9_3_a"
#| warning: false
#| code-fold: false

x <- c(5,6,7,10,12,15,18,20)
y <- c(7.4, 9.3, 10.6, 15.4, 18.1, 22.2, 24.1, 24.8)
df <- data.frame(x = x, y = y)
model <- lm(y ~ x, df)
model

plot(x, y)
abline(model)
```

如 @fig-9_2 所示，估计的线性回归方程为：

$y = 2.463 + 1.206x$ $\blacksquare$
:::